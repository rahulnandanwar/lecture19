{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why reinforcement learning ?\n",
    "\n",
    "1) Training Machine learning model requires a lot of data which might not always be available to us.\n",
    "\n",
    "2) Further the data provided might not be reliable\n",
    "\n",
    "3) Learning from small subset of actions will not help expand the vast reality of solutions that may work for a particular problem.\n",
    "\n",
    "This is going to slow the growth that technology is capable of. Machines need to learn to perform actions by themselves and not just learn off humans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is reinforcement learning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning(RL) is a type of maachine learning, in which an agent explores an environment to learn how to perform \n",
    "desired tasks by taking actions with bad outcomes.\n",
    "\n",
    "A reinforcement learning model will learn from its experience and over time it will be able to identify which actions lead to \n",
    "the best rewards.\n",
    "\n",
    "classical examples of RL are \n",
    "\n",
    "1) driverless cars\n",
    "\n",
    "2) game playing agents (Chess, Go, etc.)\n",
    "\n",
    "3) mechanical robots in factories/warehouses etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other types of Machine learning\n",
    "\n",
    "### Supervised learning\n",
    "\n",
    "Example-driven training - with labeled data of known outputs of the given inputs, a model is trained to predict \n",
    "output for new inputs.\n",
    "\n",
    "### Unsupervised learning\n",
    "\n",
    "Inference-based training - with unlabeled data without known outputs, a model is trained to identify related structures \n",
    "or similar patterns within the input data.\n",
    "\n",
    "\n",
    "<img src=\"SupervisedVsUnsupervisedVsRL.JPG\" width=\"800\" height=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### In reinforcement learning, you deal with the processes where the agent actively interacts with the environment. \n",
    "\n",
    "Whereas in supervised learning, you deal with objects or datasets. There is no interaction with the environment and given \n",
    "a dataset, you are required to predict the target.\n",
    "\n",
    "RL is an active learning, where the agent learns only by interacting. While supervised learning is passive learning, where \n",
    "the agent learns only by extracting features from a given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "Reinforcement learning is a machine learning method that is focused on autonomous decision-making by an agent in order to \n",
    "achieve specified goals through interactions with an environment. \n",
    "\n",
    "In reinforcement learning, learning is achieved through trial and error and training does not require labeled input. \n",
    "Training relies on the reward hypothesis, which posits that all goals can be achieved by maximizing a future reward after \n",
    "action sequences. In reinforcement learning, designing the reward function is important. Better-crafted reward functions result \n",
    "in better decisions from better the agent can decide what actions to take to reach the goal.\n",
    "\n",
    "For autonomous racing, the agent is a vehicle. The environment includes traveling routes and traffic conditions. \n",
    "The goal is for the vehicle to reach its destination quickly without accidents. Rewards are scores used to encourage safe and \n",
    "speedy travel to the destination. The scores penalize dangerous and wasteful driving.\n",
    "\n",
    "To encourage learning during training, the learning agent must be allowed to sometimes pursue actions that might not result in \n",
    "rewards. This is referred to as the exploration and exploitation trade-off. It helps reduce or remove the likelihood that the \n",
    "agent might be misguided into false destinations.\n",
    "\n",
    "A reinforcement learning model is an environment in which an agent acts that establishes three things: The states that the \n",
    "agent has, the actions that the agent can take, and the rewards that are received by taking action. The strategy with which the \n",
    "agent decides its action is referred to as a policy. The policy takes the environment state as input and outputs the action to \n",
    "take. In reinforcement learning, the policy is often represented by a deep neural network. We refer to this as the \n",
    "reinforcement learning model. Each training job generates one model. A model can be generated even if the training job is \n",
    "stopped early. A model is immutable, which means it cannot be modified and overwritten after it's created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### In Short, In reinforcement learning an agent interacts with an environment with an objective to maximize it's total reward.\n",
    "\n",
    "###### The agent takes an action based on the environment state and theenvironment returns the reward and next state. The agent learns from trial\n",
    "###### and error, initially taking random actions and over time identifying the rewards that lead to long-term rewards.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"reinforcement_learning_animation.png\" width=\"800\" height=\"800\">\n",
    "\n",
    "\n",
    "####                                                                      Reinforcement learning specific entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How To Train the Vehicle\n",
    "\n",
    "\n",
    "The training process is an iterative tass. In a simulator, the agent will explore the environment and gains experience. The experiences collected are used to update the model, and the updated models are used to gain more experiences.\n",
    "We will see the examples in a simplified environment to really see how the training process happens.\n",
    "\n",
    "\n",
    "<img src=\"Model_training.png\" width=\"1200\" height=\"1200\">\n",
    "\n",
    "\n",
    "###### The agent needs to explore and see where it can get the highest rewards before it can use or exploit that knowledge. As the agent gains more and more experience, it learns to stay on the central squares to get higher rewards.\n",
    "\n",
    "\n",
    "With more experience, the agent gets better and eventually can reach the destination reliably and find the highest rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent \n",
    "\n",
    "The agent is represented vehicle that needs to be trained. More specifically, it embodies the neural \n",
    "network(Or ml logic or rule based logic) that controls the vehicle, taking inputs, and deciding actions.\n",
    "\n",
    "###### Agent is the model that is being trained via reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "The environment contains a track that defines where the agent can go and what state it can be in. \n",
    "\n",
    "###### The agent explores the environment to collect data to train the underlying neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State \n",
    "\n",
    "###### A state represents a snapshot of the environment the agent is in at a point in time.\n",
    "\n",
    "So, a state is a representation of the environment at any point in time. The environment will give all the signals, but how relevant those signals are for the agent to take an action is what you have to decide. You can consider state vector as a list of features that help the agent to take an action. For each RL problem, state vector would be different.\n",
    "\n",
    "In this case, a state is an image captured by the front-facing camera on the vehicle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action\n",
    "\n",
    "###### An action is a move made by the agent in the current state. \n",
    "\n",
    "For this example move an action corresponds to a move at particular speed and steering angle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward\n",
    "\n",
    "######  The reward is the score given as feedback to the agent when it takes an action in a given state.\n",
    "\n",
    "To help the model move in the right direction, it is rewarded/points are given to it to appraise same action.\n",
    "\n",
    "In this example, the reward is returned by a reward function. In general, you define or supply a reward function to specify \n",
    "what is desirable or undesirable action for the agent to take in a given state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an RL model\n",
    "\n",
    "Training is an iterative process, In a simulator the agent explores the environment and builds up experience. The experience \n",
    "collected are used to update the neural network (Possible with ML model and simple python programme as well) periodically and the updated models are used to create more experiences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simplified Environment\n",
    "\n",
    "In this example, we want the vehicle to go from the starting point to the finish line following the shortest path.\n",
    "\n",
    "We've simplified the environment to grid of squares. Each square represents an individual state, and we will allow the \n",
    "vehicle to move up or down while facing in the direction of the goad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An episode or Policy\n",
    "\n",
    "In reinforcement training, the vehicle will start by exploreing the grid until it moves out of bounds or reaches the destination.\n",
    "\n",
    "As it drives around, the vehicle accumulates rewards from the scores we defined. This process is called an episode.\n",
    "\n",
    "###### Policy determines how an agent will behave at anytime. It acts as a mapping between Action and present state.\n",
    "\n",
    "A policy is a set of rules which helps the agent decide the action that it should take in a given state such that the agent can maximise its rewards in the long run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two types of tasks:\n",
    "\n",
    "#### Continuous: \n",
    "\n",
    "###### tasks that do not have a definite end - e.g. learning to walk, controlling a chemical plant, driving a car\n",
    "\n",
    "#### Episodic tasks: \n",
    "    \n",
    "###### tasks that have a definite end - e.g. most games (videos games, Chess, Ludo) etc. are episodic since at the end of the game the agent either wins or loses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration\n",
    "\n",
    "Reinforcement learning algorithms are trained by repeated optimization od cummulative rewards.\n",
    "\n",
    "The model will learn which action (and then subsequent actions) will result in the highest cummulative \n",
    "reward on the way to the goal.\n",
    "\n",
    "Learning doesn't just happen on the first go; it takes some iteration. First, the agent needs to explore \n",
    "and see where it can get the highest rewards, before it can exploit that knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration\n",
    "\n",
    "As the agent gains more and more experiences, it learns to stay on the central squares to get higher rewards.\n",
    "\n",
    "If we plot the total reward from each episode, we can see how the model performs and improves over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploitation and Convergence\n",
    "\n",
    "With more experience, the agent gets better and eventually is able to reach the destination reliably.\n",
    "\n",
    "###### Depending on the exploration-exploitation strategy, the vehicle may still have a small probablity of taking random actions to explore the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can summarise the objectives of the RL agent in the following manner:\n",
    "\n",
    "The objective of episodic tasks is to find such a sequence of actions that will make the majority of episodes successful.\n",
    "\n",
    "For continuing tasks, break it into multiple episodes and then find out actions that maximise the average rewards earned from those episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration-Exploitation Tradeoff: \n",
    "\n",
    "Exploiting an action is fine if you have exhaustively explored all the actions from a given state. But, this is generally not the case in real-life situations. In most scenarios, you would have explored only a small fraction of all possible actions. What if there exists an action that can get you a lottery? Wouldn’t you go exploring more? But at the same time, you also don’t want to lose out on the benefits of the current action, in case you don’t find good options while exploring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov State\n",
    "\n",
    "###### The Markov assumption states that the current state contains all the necessary information about all the past states the agent was in and all the past actions the agent took. It assumes that the current state is sufficient for taking the next action. (It is independent of what happend in the past)\n",
    "\n",
    "###### You can consider a Markov state as some function of the knowledge base that captures all relevant information from the knowledge base. \n",
    "And once ‘Markov state’ is known, the knowledge base can be thrown away. What action the agent needs to take next or what possible state an agent can land on given he has taken an action - all of this can be determined from the Markov state.\n",
    "\n",
    "\n",
    "###### All these processes that work in accordance to Markov property are called Markov Decision Processes (popularly called MDPs). The word ‘Decision’ in MDP takes into account actions taken by the agent in a given Markov state. MDP is the formal name of a sequential decision-making process. All the RL problems set its ground on MDPs, i.e., work on the assumption of the Markov property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process (MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these processes that work in accordance to Markov property are called Markov Decision Processes (popularly called MDPs). The word ‘Decision’ in MDP takes into account actions taken by the agent in a given Markov state. MDP is the formal name of a sequential decision-making process. All the RL problems set its ground on MDPs, i.e., work on the assumption of the Markov property. \n",
    "\n",
    "###### The mathematical framework for defining a solution in reinforcement learning scenario is called Markov Decision Process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model of the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are broadly two types of frameworks in RL: model-based and model-free.\n",
    "\n",
    "###### In model-based methods, it is possible to learn what is called a model of the environment, i.e. a model which maps the consequences (next state, reward) of taking an action in a state.\n",
    "\n",
    "In model-free methods, it is not possible to learn an explicit model (which is a more realistic case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In most cases, the environment is stochastic, i.e., most of the times you will see different rewards and states after taking a particular action in a particular state. Therefore, the model is represented as: \n",
    "\n",
    "\n",
    "######                                                p(s′,r|s,a)\n",
    "\n",
    "\n",
    "It is the probabilistic distribution of finding the agent in state 's' and reaping the reward r, given a particular action a is taken in a particular state s. This is known as the model of the environment.\n",
    "\n",
    " \n",
    "\n",
    "In most real-world scenarios, you wouldn’t know what exactly the model of the environment is. You implicitly infer about the model from the observations and the RL techniqes used to solve such problems are called model-free. \n",
    "\n",
    " \n",
    "\n",
    "So, the objective of an RL agent is to find the optimal policy either using the explicit model (model-based) or by implicitly inferring the model from the actions taken from various states (model-free). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With grounds set for the reinforcement learning problem, let’s start with some basic equations relating :\n",
    "\n",
    "###### Policy \n",
    "######                                          π(a|s) -- Distribution over possible actions given state s\n",
    "\n",
    "###### State-value: \n",
    "######                                           v(s) -- Value function at state s \n",
    "\n",
    "###### q-value: \n",
    "######                                           q(s,a) -- Action value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run a large number of episodes (play a large number of games) from the state s following the policy π, the expected total reward will be the value of that state under the policy π. Similarly, you can think of q-values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First basic equation of RL -- Equation for State Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The intuition of this equation is that the value function of a state s is the weighted sum over all action-value functions for different actions that can be taken from state s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"First_equation_of_RL.PNG\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Equaition of RL -- Equation Action Value Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The intuition of this equation is that the action-value function for an action a taken from state s\n",
    "###### is the weighted sum of total reward (immediate reward + discounted future reward) over model probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Second_equation_of_RL.PNG\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarise, \n",
    "\n",
    "###### The state-value function v(s) is the total reward an agent can expect if it is in state s\n",
    "\n",
    "###### The action-value function q(s, a) is the total reward an agent can expect if it performs an action ‘a’ when it is in the state ‘s’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-Free Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These equations can be solved if the model of the environment \n",
    "\n",
    "### p(s′,r|s,a)\n",
    "\n",
    "is available. In most real-life scenarios with large state and action spaces, the model of the environment is not available (i.e. model-free methods). In such cases, you can compute the q-function using the fact that it is the expected value of the total reward:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"model_free.PNG\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the expected value, you take multiple episodes where the agent was in state ‘s’ and took the action ‘a’. For this particular state-action pair, you will get different (s', reward) pairs for different episodes. To get an estimate of the expected value of this state-action pair - \n",
    "\n",
    "### qπ(s,a)\n",
    "\n",
    "- you take the average of the different rewards you get via these different episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Equations of Optimality\n",
    "\n",
    "Optimal policy refers to finding the best action a in state s\n",
    "\n",
    "In this case, as is the case for many MDPs, the optimal policy is deterministic, i.e., there is only one optimal action to take \n",
    "in each state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### two basic steps for solving any RL problem\n",
    "\n",
    "All these equations work together to arrive at optimal policy and optimal value functions. You will learn to solve these equations to achieve optimal policy in the next module. For now, it's important to reiterate that there are broadly two steps involved in arriving at optimal policy and state-value functions:\n",
    "\n",
    "#### Policy Evaluation: \n",
    "\n",
    "Say you know a policy and you want to evaluate how good it is, i.e., compute the state-value functions for the existing policy\n",
    "\n",
    "#### Policy Improvement:\n",
    "\n",
    "Say you know the value function for each state and you want to improve the policy which was used to compute these value functions, i.e., improve the policy using the existing state value function. This new policy could not have been found directly from the old policy without calculating those value functions.\n",
    "\n",
    "As the name suggests, policy improvement is done to maximise the total rewards, by finding the best action for each state.\n",
    "\n",
    "#### two basic approaches to solve an RL problem:\n",
    "\n",
    "Policy Iteration and\n",
    "\n",
    "Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Example\n",
    "The process of learning an optimal policy is analogous to learning to play, say, Chess. You want to learn the optimal policy (i.e. learn to play well), and to do that you need to know the ‘values’ of the various states (i.e. get an intuition of which states are good). So you start playing with some policy (the initial ones are likely to be bad) and compute the ‘state-values’ under that policy. Once you have played enough games under a policy, you can try changing your policy, compare that with other policies, and gradually improve your policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Based Methods (know the model of the environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dynamic Programming\n",
    "\n",
    "Dynamic Programming (DP) is a method of solving complex problems by breaking them into sub-problems. It solves the sub-problems and stores the solution of sub-problems, so when next time same subproblem arises, one can simply look up the previously computed solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy iteration \n",
    "\n",
    "###### Initialize a policy randomly\n",
    "\n",
    "###### Policy Evaluation:\n",
    "Measure how good that policy is by calculating the state-values vπ(s)\n",
    "\n",
    " corresponding to all the states until the state-values are converged.\n",
    "Also known as the prediction problem.\n",
    "\n",
    "Policy evaluation refers to the iterative computation of the value functions for a given policy.\n",
    "\n",
    "###### Policy Improvement:\n",
    "Make changes in the policy you evaluated in the policy evaluation step.\n",
    "Also known as the control problem.\n",
    "\n",
    "Policy improvement refers to the finding an improved policy given the value function for an existing policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "\n",
    "In value iteration, for every state update, you’re doing a policy improvement, i.e., updating the state value by picking the most greedy action using current estimates of state-values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalised Policy Iteration (GPI)\n",
    "\n",
    "Generalised Policy Iteration (GPI) is a class of algorithms that pans out the entire range of strategies that fall between Policy Iteration & Value Iteration. A GPI is basically the following two steps running in a loop\n",
    "\n",
    "Updates based on current values\n",
    "\n",
    "Policy improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Free Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo Methods\n",
    "\n",
    "Monte-Carlo method is based on the concept of the law of large numbers.\n",
    "\n",
    "The law of large numbers says that if you take a very large sample, it will give similar results as to what you would get if you would have known the actual distribution of the samples.\n",
    "\n",
    "The expected value of a random variable is the weighted average over the probabilistic distribution values. It can also be thought of as the average (or mean) of a (~infinitely) large enough sample drawn from the same distribution.\n",
    "\n",
    "### E[X]=c1X1 +c2X2+...such that c1+c2+...=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Monte-Carlo methods require only knowledge base (history/past experiences)—sample sequences of (states, actions and rewards) from the interaction with the environment, and no actual model of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo Prediction\n",
    "\n",
    "Prediction and control are two integral steps to solve any Reinforcement learning problem.\n",
    "\n",
    "###### Prediction - \n",
    "\n",
    "evaluating the value function/policy\n",
    "\n",
    "###### Control - \n",
    "\n",
    "improving the policy basis the state-value function estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Monte-Carlo prediction problem is to estimate \n",
    "\n",
    "qπ(s,a), i.e. the expected total reward the agent can get after taking an action a from state s.\n",
    "\n",
    "    1) For estimating this, you need to run multiple episodes\n",
    "    \n",
    "    2) Track the total reward that you get in every episode corresponding to this (s, a) pair\n",
    "    \n",
    "    3) The estimated action-value is then given by \n",
    "\n",
    "                            qπ(s,a) ≈ ∑rin\n",
    " \n",
    "\n",
    "The most important thing to ensure is that, while following the policy π, each state-action pair should be visited enough number of times to get a true estimate of qπ(s,a). For this, you need to keep a condition known as the exploring starts. It states that every state-action pair should have a non-zero probability of being the starting pair. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Monte-Carlo, Temporal Difference (TD) methods also learn directly from experience without an explicit model of the environment.\n",
    "\n",
    "The major difference between these two methods is:\n",
    "\n",
    "    1) In Monte Carlo methods, you need to wait until the end of the episode and then update the q-value\n",
    "    2) In TD methods, you can update the value after every few time steps. This update mechanism is often advantageous since it gives the agent early signals if some of the eventual states are disastrous, and so the agent avoids that path altogether."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, any TD algorithm could be written as:\n",
    "\n",
    "#### new q-value = old q-value + α(current value - old q-value)\n",
    "\n",
    "#### new q-value = 100 + α(70-100)\n",
    "\n",
    " \n",
    "\n",
    "Now, α is the learning rate that decides how much importance you give to your current value.\n",
    "\n",
    "###### If α is too low, it means that you value the incremental learning less\n",
    "###### If α is high, it means that you value the incremental learning more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "\n",
    "Q-learning learns the optimal policy 'relentlessly' because the estimates of q-values are updated based on the q-value of the next state-action pair assuming the greediest action will be taken subsequently.\n",
    "\n",
    " \n",
    "\n",
    "If there is a risk of a large negative reward close to the optimal path, Q-learning will tend to trigger that reward while exploring. And in practice, if the mistakes are costly, you don't want Q-learning to explore more of these negative rewards. You will want something more conservative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update equation for Q-learning is: \n",
    "\n",
    "<img src=\"Q_learning.PNG\" width=\"600\" height=\"600\">                      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Gym\n",
    "\n",
    "Founded by Elon Musk and Sam Altman, OpenAI is a non-profit research company that is focussed on building out AI algorithms. OpenAI  Gym is a toolkit for developing and comparing reinforcement learning algorithms. Gym provides different environments to implement various reinforcement learning algorithms.\n",
    "\n",
    " \n",
    "\n",
    "This segment is to make you familiar with OpenAI Gym. You'll learn to use these environments and try running RL algorithms on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
